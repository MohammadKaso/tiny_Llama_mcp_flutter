import Foundation
import Metal
import MetalKit
import Flutter
import UIKit

// MARK: - MLC-LLM C++ Bridge
// These functions will be implemented via C++ bridging
@_silgen_name("mlc_llm_create_engine")
func mlc_llm_create_engine(_ model_path: UnsafePointer<CChar>) -> UnsafeMutableRawPointer?

@_silgen_name("mlc_llm_generate")
func mlc_llm_generate(_ engine: UnsafeMutableRawPointer, 
                     _ prompt: UnsafePointer<CChar>,
                     _ max_tokens: Int32,
                     _ temperature: Float,
                     _ callback: @convention(c) (UnsafePointer<CChar>?) -> Void) -> Int32

@_silgen_name("mlc_llm_destroy_engine")
func mlc_llm_destroy_engine(_ engine: UnsafeMutableRawPointer)

// MARK: - Real MLC LLM Engine Implementation
class MLCLlamaEngine: NSObject {
    private var isInitialized = false
    private var modelPath: String = ""
    private var device: MTLDevice?
    
    // Real MLC-LLM engine instance
    private var mlcEngine: UnsafeMutableRawPointer?
    
    // Model configuration from compilation
    private let vocabSize: Int = 32000
    private let contextSize: Int = 2048
    private let maxBatchSize: Int = 80
    
    override init() {
        self.device = MTLCreateSystemDefaultDevice()
        super.init()
        print("üîß Initializing REAL MLCLlamaEngine with compiled TinyLlama model")
    }
    
    deinit {
        if let engine = mlcEngine {
            mlc_llm_destroy_engine(engine)
        }
    }
    
    func initialize() async throws {
        guard !isInitialized else { return }
        
        print("üöÄ Initializing MLC-LLM engine with real compiled model...")
        
        // Get the TinyLlama model path from bundle
        guard let modelBundle = Bundle.main.path(forResource: "TinyLlama", ofType: "bundle"),
              let bundle = Bundle(path: modelBundle) else {
            throw NSError(domain: "MLCLlamaEngine", code: -1, 
                         userInfo: [NSLocalizedDescriptionKey: "TinyLlama model bundle not found"])
        }
        
        // Look for the compiled model files
        guard let modelConfigPath = bundle.path(forResource: "mlc-chat-config", ofType: "json") else {
            throw NSError(domain: "MLCLlamaEngine", code: -2,
                         userInfo: [NSLocalizedDescriptionKey: "Model config not found in bundle"])
        }
        
        self.modelPath = URL(fileURLWithPath: modelConfigPath).deletingLastPathComponent().path
        
        // Initialize the real MLC-LLM engine
        self.mlcEngine = mlc_llm_create_engine(modelPath.cString(using: .utf8))
        
        guard mlcEngine != nil else {
            throw NSError(domain: "MLCLlamaEngine", code: -3,
                         userInfo: [NSLocalizedDescriptionKey: "Failed to create MLC-LLM engine"])
        }
        
        isInitialized = true
        
        print("‚úÖ MLC-LLM engine initialized successfully!")
        print("üìä Model: TinyLlama-1.1B-Chat-v1.0-q4f16_1-MLC")
        print("üíæ Memory usage: 716.24 MB (590.24 MB parameters + 126.00 MB temporary buffer)")
        print("‚ö° Metal acceleration: \(device?.name ?? "Unknown GPU")")
        print("üéØ Context size: \(contextSize) tokens")
        print("üìà Vocab size: \(vocabSize)")
    }
    
    func generate(prompt: String, maxTokens: Int = 2048, temperature: Float = 0.7) async throws -> [String] {
        guard isInitialized, let engine = mlcEngine else {
            throw NSError(domain: "MLCLlamaEngine", code: -4,
                         userInfo: [NSLocalizedDescriptionKey: "Engine not initialized"])
        }
        
        print("üîÑ Running REAL TinyLlama inference with compiled model")
        print("üìù Prompt: \"\(prompt)\"")
        print("üéõÔ∏è Max tokens: \(maxTokens), Temperature: \(temperature)")
        
        var tokens: [String] = []
        let semaphore = DispatchSemaphore(value: 0)
        
        // Token callback for streaming
        let tokenCallback: @convention(c) (UnsafePointer<CChar>?) -> Void = { tokenPtr in
            guard let tokenPtr = tokenPtr else { return }
            let token = String(cString: tokenPtr)
            tokens.append(token)
        }
        
        // Call the real MLC-LLM inference
        let result = mlc_llm_generate(engine, 
                                     prompt.cString(using: .utf8), 
                                     Int32(maxTokens), 
                                     temperature, 
                                     tokenCallback)
        
        if result != 0 {
            throw NSError(domain: "MLCLlamaEngine", code: Int(result),
                         userInfo: [NSLocalizedDescriptionKey: "MLC-LLM generation failed"])
        }
        
        print("‚úÖ Generated \(tokens.count) tokens using real TinyLlama model")
        return tokens
    }
    
    private func splitIntoStreamingTokens(_ text: String) -> [String] {
        // Split text into word-level tokens for streaming
        let words = text.components(separatedBy: .whitespacesAndNewlines)
        var tokens: [String] = []
        
        for (index, word) in words.enumerated() {
            if !word.isEmpty {
                tokens.append(word)
                if index < words.count - 1 {
                    tokens.append(" ")
                }
            }
        }
        
        return tokens
    }
} 